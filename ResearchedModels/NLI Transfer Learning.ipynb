{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d72962e",
   "metadata": {},
   "source": [
    "# Transfer Learning Approach for Cross-Lingual NLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a51bbdc",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e523102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import gdown\n",
    "import time\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# set a seed value\n",
    "torch.manual_seed(205)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "import wandb\n",
    "\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from transformers import PreTrainedModel, PretrainedConfig\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification #, XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from transformers import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7da96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_TYPE = 'xlm-roberta-base'\n",
    "TOKENIZER_TYPE = 'bert-base-multilingual-cased'\n",
    "MBERT_TYPE = 'bert-base-multilingual-cased'\n",
    "MODEL_TYPE = 'jalaluddin94/nli_mbert'\n",
    "MODEL_PATH = 'D:/Training/Machine Learning/NLP/NLI/saved_models/Indo-Javanese-NLI/ResearchedModels/'\n",
    "\n",
    "L_RATE = 3e-6\n",
    "STUDENT_LRATE = 3e-6\n",
    "MAX_LEN = 512\n",
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 8\n",
    "BATCH_NORM_EPSILON = 1e-5\n",
    "LAMBDA_L2 = 3e-5\n",
    "\n",
    "HF_TOKEN = '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'\n",
    "\n",
    "NUM_CORES = os.cpu_count() - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db31298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_API_KEY=97b170d223eb55f86fe1fbf9640831ad76381a74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjalaluddin-94\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %env WANDB_NOTEBOOK_NAME=/home/sagemaker-user/PPT/BERT_BiLSTM_Game_Review.ipynb\n",
    "%env WANDB_API_KEY=97b170d223eb55f86fe1fbf9640831ad76381a74\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af0da98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_LOG_MODEL='end'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Training\\Machine Learning\\NLP\\NLI\\wandb\\run-20230613_023205-t00xi5e3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jalaluddin-94/javanese_nli/runs/t00xi5e3' target=\"_blank\">transfer-learning-paper</a></strong> to <a href='https://wandb.ai/jalaluddin-94/javanese_nli' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jalaluddin-94/javanese_nli' target=\"_blank\">https://wandb.ai/jalaluddin-94/javanese_nli</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jalaluddin-94/javanese_nli/runs/t00xi5e3' target=\"_blank\">https://wandb.ai/jalaluddin-94/javanese_nli/runs/t00xi5e3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %env WANDB_PROJECT=javanese_nli\n",
    "%env WANDB_LOG_MODEL='end'\n",
    "run = wandb.init(\n",
    "  project=\"javanese_nli\",\n",
    "  notes=\"Experiment transfer learning on Bandyopadhyay's paper\",\n",
    "  name=\"transfer-learning-paper\",\n",
    "  tags=[\"transferlearning\", \"bandyopadhyay\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1e38554",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_AGENT_MAX_INITIAL_FAILURES\"]=\"1024\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf2f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_AGENT_DISABLE_FLAPPING\"]=\"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21714959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed2640d",
   "metadata": {},
   "source": [
    "## Download and Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52523ba",
   "metadata": {},
   "source": [
    "### Download Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bedd4315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uri = \"https://drive.google.com/uc?id=1aE9w2rqgW-j3PTgjnmHDjulNwp-Znb6i\"\n",
    "# output = \"dataset/indo_java_nli_training.csv\"\n",
    "# if not os.path.exists(\"dataset/\"):\n",
    "#   os.makedirs(\"dataset/\")\n",
    "# gdown.download(url=uri, output=output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aaa3ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uri = \"https://drive.google.com/uc?id=1YlQ9_8CvQbTSb5-2BjIfiYT-cy7pe6YM\"\n",
    "# output = \"dataset/indo_java_nli_validation.csv\"\n",
    "# if not os.path.exists(\"dataset/\"):\n",
    "#   os.makedirs(\"dataset/\")\n",
    "# gdown.download(url=uri, output=output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1952d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uri = \"https://drive.google.com/uc?id=1Zz_rHeI7fPUuA04zt9gCWyl5RYhrYPn0\"\n",
    "# output = \"dataset/indo_java_nli_testing.csv\"\n",
    "# if not os.path.exists(\"dataset/\"):\n",
    "#   os.makedirs(\"dataset/\")\n",
    "# gdown.download(url=uri, output=output, quiet=False, fuzzy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00585f5a",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Student "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41501e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"D:/Training/Machine Learning/Datasets/NLI/IndoJavaNLI/indojavanesenli-train.csv\", sep='\\t')\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True) #shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c108af3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Edisi novel di Britania Raya memenangkan Man B...</td>\n",
       "      <td>novel wis nduweni edisi sing diterbitkan neng ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sarana olahragapun tidak luput dari pemikiran ...</td>\n",
       "      <td>lapangan sapak bola digugah.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Penduduk kabupaten Raja Ampat mayoritas memelu...</td>\n",
       "      <td>kajaba agama kristen, akeh warga kabupaten raj...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Petisi yang diinisiasi oleh Tyler Sigmon itu m...</td>\n",
       "      <td>tyler sigmon ora tau menginisiasi siji petisi.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Untuk video, Air 2 bisa merekam video 4K 60fps...</td>\n",
       "      <td>banyu 2 isa nduweni resolusi 5k.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  Edisi novel di Britania Raya memenangkan Man B...   \n",
       "1  Sarana olahragapun tidak luput dari pemikiran ...   \n",
       "2  Penduduk kabupaten Raja Ampat mayoritas memelu...   \n",
       "3  Petisi yang diinisiasi oleh Tyler Sigmon itu m...   \n",
       "4  Untuk video, Air 2 bisa merekam video 4K 60fps...   \n",
       "\n",
       "                                          hypothesis  label  \n",
       "0  novel wis nduweni edisi sing diterbitkan neng ...      0  \n",
       "1                       lapangan sapak bola digugah.      0  \n",
       "2  kajaba agama kristen, akeh warga kabupaten raj...      1  \n",
       "3     tyler sigmon ora tau menginisiasi siji petisi.      2  \n",
       "4                   banyu 2 isa nduweni resolusi 5k.      2  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_student = pd.DataFrame()\n",
    "df_train_student[\"premise\"] = df_train[\"premise\"]\n",
    "df_train_student[\"hypothesis\"] = df_train[\"jv_hypothesis_mongo\"]\n",
    "df_train_student[\"label\"] = df_train[\"label\"]\n",
    "df_train_student.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4370c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = pd.read_csv(\"D:/Training/Machine Learning/Datasets/NLI/IndoJavaNLI/indojavanesenli-valid.csv\", sep='\\t')\n",
    "df_valid = df_valid.sample(frac=1).reset_index(drop=True) #shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3fd1f962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Selain itu pengamanan ketat juga disiagakan di...</td>\n",
       "      <td>ora ana pangamanan sing kedelok neng kutha ban...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rumah makan ini biasanya dijadikan tempat tran...</td>\n",
       "      <td>omah madhang iki ora mengenke tunggangan bus k...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Utama adalah salah satu kelurahan di Kecamatan...</td>\n",
       "      <td>cimahi kidul ngrupakne siji kecamatan neng kut...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dewan Riset Nasional yang dikelola pemerintah ...</td>\n",
       "      <td>itali isih upadi atos kanggo mengurangi penyeb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pasangan nomor satu dunia asal Indonesia, Marc...</td>\n",
       "      <td>pasangan nomor siji donya seka indonesia, marc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  Selain itu pengamanan ketat juga disiagakan di...   \n",
       "1  Rumah makan ini biasanya dijadikan tempat tran...   \n",
       "2  Utama adalah salah satu kelurahan di Kecamatan...   \n",
       "3  Dewan Riset Nasional yang dikelola pemerintah ...   \n",
       "4  Pasangan nomor satu dunia asal Indonesia, Marc...   \n",
       "\n",
       "                                          hypothesis  label  \n",
       "0  ora ana pangamanan sing kedelok neng kutha ban...      2  \n",
       "1  omah madhang iki ora mengenke tunggangan bus k...      2  \n",
       "2  cimahi kidul ngrupakne siji kecamatan neng kut...      0  \n",
       "3  itali isih upadi atos kanggo mengurangi penyeb...      1  \n",
       "4  pasangan nomor siji donya seka indonesia, marc...      0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid_student = pd.DataFrame()\n",
    "df_valid_student[\"premise\"] = df_valid[\"premise\"]\n",
    "df_valid_student[\"hypothesis\"] = df_valid[\"jv_hypothesis_mongo\"]\n",
    "df_valid_student[\"label\"] = df_valid[\"label\"]\n",
    "df_valid_student.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c34790a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"D:/Training/Machine Learning/Datasets/NLI/IndoJavaNLI/indojavanesenli-test.csv\", sep='\\t')\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True) #shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf1c3fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Esarhadon kembali ke ibu kota Niniwe dan menga...</td>\n",
       "      <td>esarhado lair neng siji kutha terpencil.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pengunjung dapat menikmati kawah serta kawasan...</td>\n",
       "      <td>pengunjung kudu ngetoke dhuwit kanggo menaiki ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Menurut Le Figaro, rencana pemerintah ini bisa...</td>\n",
       "      <td>pamerentah mengalokasikan 15,5 yuta euro saka ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asia Tenggara memiliki letak strategis dan sum...</td>\n",
       "      <td>asia nduweni sumber daya alam sing akeh.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Memetika pada umumnya mengadopsi konsep dari t...</td>\n",
       "      <td>memetika nggunakne model matematika.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0  Esarhadon kembali ke ibu kota Niniwe dan menga...   \n",
       "1  Pengunjung dapat menikmati kawah serta kawasan...   \n",
       "2  Menurut Le Figaro, rencana pemerintah ini bisa...   \n",
       "3  Asia Tenggara memiliki letak strategis dan sum...   \n",
       "4  Memetika pada umumnya mengadopsi konsep dari t...   \n",
       "\n",
       "                                          hypothesis  label  \n",
       "0           esarhado lair neng siji kutha terpencil.      1  \n",
       "1  pengunjung kudu ngetoke dhuwit kanggo menaiki ...      1  \n",
       "2  pamerentah mengalokasikan 15,5 yuta euro saka ...      1  \n",
       "3           asia nduweni sumber daya alam sing akeh.      1  \n",
       "4               memetika nggunakne model matematika.      1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_student = pd.DataFrame()\n",
    "df_test_student[\"premise\"] = df_test[\"premise\"]\n",
    "df_test_student[\"premise\"] = df_test_student[\"premise\"].astype(str)\n",
    "df_test_student[\"hypothesis\"] = df_test[\"jv_hypothesis_mongo\"]\n",
    "df_test_student[\"hypothesis\"] = df_test_student[\"hypothesis\"].astype(str)\n",
    "df_test_student[\"label\"] = df_test[\"label\"]\n",
    "df_test_student.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c463c38",
   "metadata": {},
   "source": [
    "### Prepare Dataset for Teacher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15cd2f4",
   "metadata": {},
   "source": [
    "Dataset from teacher will be from \"IndoNLI\", and using Indonesian only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51c728fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Makamnya sempat tak diketahui, lalu Zain bin A...</td>\n",
       "      <td>Makamnya selalu terkenal.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tanjung Johor adalah salah satu kelurahan di K...</td>\n",
       "      <td>Provinsi Jambi memiliki kelurahan bernama Tanj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daerah yang dikontrol oleh Bogd Khaan kira-kir...</td>\n",
       "      <td>Bogd Khaan besar di Mongolia.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pada hari Selasa, Amerika Serikat bisa saja me...</td>\n",
       "      <td>Amerika Serikat tidak mungkin memililh preside...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Meskipun isu keamanan membuat beberapa perusah...</td>\n",
       "      <td>Eric Yuan gagal meyakinkan investor.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10325</th>\n",
       "      <td>Norman adalah salah satu teman baik ku, jadi s...</td>\n",
       "      <td>Norman adalah teman baik ku.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10326</th>\n",
       "      <td>Arsenal sukses mencatatkan tiga poin krusial y...</td>\n",
       "      <td>Arsenal mencatatkan tiga poin krusial dalam la...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10327</th>\n",
       "      <td>Bale mulai tersisih dari tim inti Real Madrid ...</td>\n",
       "      <td>Bale sakit akibat ia cedera.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10328</th>\n",
       "      <td>Sayangnya, sebuah kecelakaan fatal telah meren...</td>\n",
       "      <td>Ia meninggal di tahun 1994.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10329</th>\n",
       "      <td>Minggu lalu, Google Meet menambahkan kemampuan...</td>\n",
       "      <td>Google meet menambahkan kemampuan yang berbeda...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10330 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 premise  \\\n",
       "0      Makamnya sempat tak diketahui, lalu Zain bin A...   \n",
       "1      Tanjung Johor adalah salah satu kelurahan di K...   \n",
       "2      Daerah yang dikontrol oleh Bogd Khaan kira-kir...   \n",
       "3      Pada hari Selasa, Amerika Serikat bisa saja me...   \n",
       "4      Meskipun isu keamanan membuat beberapa perusah...   \n",
       "...                                                  ...   \n",
       "10325  Norman adalah salah satu teman baik ku, jadi s...   \n",
       "10326  Arsenal sukses mencatatkan tiga poin krusial y...   \n",
       "10327  Bale mulai tersisih dari tim inti Real Madrid ...   \n",
       "10328  Sayangnya, sebuah kecelakaan fatal telah meren...   \n",
       "10329  Minggu lalu, Google Meet menambahkan kemampuan...   \n",
       "\n",
       "                                              hypothesis  label  \n",
       "0                              Makamnya selalu terkenal.      2  \n",
       "1      Provinsi Jambi memiliki kelurahan bernama Tanj...      0  \n",
       "2                          Bogd Khaan besar di Mongolia.      1  \n",
       "3      Amerika Serikat tidak mungkin memililh preside...      2  \n",
       "4                   Eric Yuan gagal meyakinkan investor.      2  \n",
       "...                                                  ...    ...  \n",
       "10325                       Norman adalah teman baik ku.      0  \n",
       "10326  Arsenal mencatatkan tiga poin krusial dalam la...      0  \n",
       "10327                       Bale sakit akibat ia cedera.      1  \n",
       "10328                        Ia meninggal di tahun 1994.      0  \n",
       "10329  Google meet menambahkan kemampuan yang berbeda...      2  \n",
       "\n",
       "[10330 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train_t = pd.DataFrame()\n",
    "df_train_t[\"premise\"] = df_train[\"premise\"]\n",
    "df_train_t[\"hypothesis\"] = df_train[\"hypothesis\"]\n",
    "df_train_t[\"label\"] = df_train[\"label\"]\n",
    "df_train_t = df_train_t.sample(frac=1).reset_index(drop=True)\n",
    "display(df_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aebc677b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count per class train:\n",
      "0    3476\n",
      "2    3439\n",
      "1    3415\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Count per class train:\") \n",
    "print(df_train_t['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66c4ed73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pada pukul 7:00 pagi tanggal 21 November 1980,...</td>\n",
       "      <td>Kebakaran restoran hotel di Deli terjadi kuran...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bagi Anda yang memiliki hobi menantang seperti...</td>\n",
       "      <td>Banyak wisata mendaki di Bogor.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tim Korea Selatan berhasil menjuarai turnamen ...</td>\n",
       "      <td>Tim Korea Selatan sangat berbakat.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Dokter tidak memiliki alat-alat ini, mereka h...</td>\n",
       "      <td>Dokter memiliki lembar resep dan suntikan berd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dengan mencampur kopi dengan susu, maka akan l...</td>\n",
       "      <td>Kopi dicampur susu lebih aman untuk dikonsumsi.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2192</th>\n",
       "      <td>Film \"Merah Putih Memanggil\" yang disutradarai...</td>\n",
       "      <td>Film 'Merah Putih Memanggil' tayang di bioskop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2193</th>\n",
       "      <td>Bank Indonesia (BI) mencatat posisi cadangan d...</td>\n",
       "      <td>Posisi cadangan devisi Indonesia pada akhir Ag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2194</th>\n",
       "      <td>Adapun batuk pada malam hari dapat menjadi per...</td>\n",
       "      <td>Terlalu lama berada di kondisi dingin bisa men...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2195</th>\n",
       "      <td>Adi mengingatkan tersangka ini terkait anggara...</td>\n",
       "      <td>Kasus korupsi KB II belum dapat dibuktikan.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>Sebaliknya, hard rock lebih berasal dari blues...</td>\n",
       "      <td>Blues rock umumnya dimainkan lebih keras dari ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2197 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                premise  \\\n",
       "0     Pada pukul 7:00 pagi tanggal 21 November 1980,...   \n",
       "1     Bagi Anda yang memiliki hobi menantang seperti...   \n",
       "2     Tim Korea Selatan berhasil menjuarai turnamen ...   \n",
       "3     \"Dokter tidak memiliki alat-alat ini, mereka h...   \n",
       "4     Dengan mencampur kopi dengan susu, maka akan l...   \n",
       "...                                                 ...   \n",
       "2192  Film \"Merah Putih Memanggil\" yang disutradarai...   \n",
       "2193  Bank Indonesia (BI) mencatat posisi cadangan d...   \n",
       "2194  Adapun batuk pada malam hari dapat menjadi per...   \n",
       "2195  Adi mengingatkan tersangka ini terkait anggara...   \n",
       "2196  Sebaliknya, hard rock lebih berasal dari blues...   \n",
       "\n",
       "                                             hypothesis  label  \n",
       "0     Kebakaran restoran hotel di Deli terjadi kuran...      2  \n",
       "1                       Banyak wisata mendaki di Bogor.      1  \n",
       "2                    Tim Korea Selatan sangat berbakat.      0  \n",
       "3     Dokter memiliki lembar resep dan suntikan berd...      0  \n",
       "4       Kopi dicampur susu lebih aman untuk dikonsumsi.      0  \n",
       "...                                                 ...    ...  \n",
       "2192  Film 'Merah Putih Memanggil' tayang di bioskop...      1  \n",
       "2193  Posisi cadangan devisi Indonesia pada akhir Ag...      0  \n",
       "2194  Terlalu lama berada di kondisi dingin bisa men...      1  \n",
       "2195        Kasus korupsi KB II belum dapat dibuktikan.      1  \n",
       "2196  Blues rock umumnya dimainkan lebih keras dari ...      2  \n",
       "\n",
       "[2197 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_valid_t = pd.DataFrame()\n",
    "df_valid_t[\"premise\"] = df_valid[\"premise\"]\n",
    "df_valid_t[\"hypothesis\"] = df_valid[\"hypothesis\"]\n",
    "df_valid_t[\"label\"] = df_valid[\"label\"]\n",
    "df_valid_t = df_valid_t.sample(frac=1).reset_index(drop=True)\n",
    "display(df_valid_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "edefebd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count per class valid:\n",
      "0    807\n",
      "2    749\n",
      "1    641\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Count per class valid:\") \n",
    "print(df_valid_t['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "827c72ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Analisis menunjukkan hubungan langsung antara ...</td>\n",
       "      <td>Tidak terdapat analisis yang melibatkan jumlah...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bangkok terpilih sebagai tuan rumah Piala Thom...</td>\n",
       "      <td>Bangkok terpilih sebagai tuan rumah Piala Thom...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pengelola Nama Domain Internet Indonesia (Pand...</td>\n",
       "      <td>Pandi sering meluncurkan domain.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pada tahun 1933, penduduknya berjumlah 19.000 ...</td>\n",
       "      <td>Pada tahun 1933, penduduknya berjumlah 19.000 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wangi adalah sebuah desa yang berada di kecama...</td>\n",
       "      <td>Terdapat sebuah desa bernama desa Wangi.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2196</th>\n",
       "      <td>Lucasfilm resmi mengungkapkan sinopsis film le...</td>\n",
       "      <td>Sinopsis film lepas Star Wars tidak diungkapka...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>Asal nama perusahaan tersebut tidak diketahui ...</td>\n",
       "      <td>Reruntuhan tersebut ditemukan saat penghancura...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2198</th>\n",
       "      <td>Sewaktu album ini direkam, Yuko Hara mengambil...</td>\n",
       "      <td>Yuko Hara tidak pernah mengambil cuti.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2199</th>\n",
       "      <td>Oerip menentang kebijakan pemerintah yang dian...</td>\n",
       "      <td>Oerip menilai kebijakan pemerintah.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2200</th>\n",
       "      <td>Berikutnya, mereka membaca hal-hal yang mengge...</td>\n",
       "      <td>Mental positif penting.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2201 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                premise  \\\n",
       "0     Analisis menunjukkan hubungan langsung antara ...   \n",
       "1     Bangkok terpilih sebagai tuan rumah Piala Thom...   \n",
       "2     Pengelola Nama Domain Internet Indonesia (Pand...   \n",
       "3     Pada tahun 1933, penduduknya berjumlah 19.000 ...   \n",
       "4     Wangi adalah sebuah desa yang berada di kecama...   \n",
       "...                                                 ...   \n",
       "2196  Lucasfilm resmi mengungkapkan sinopsis film le...   \n",
       "2197  Asal nama perusahaan tersebut tidak diketahui ...   \n",
       "2198  Sewaktu album ini direkam, Yuko Hara mengambil...   \n",
       "2199  Oerip menentang kebijakan pemerintah yang dian...   \n",
       "2200  Berikutnya, mereka membaca hal-hal yang mengge...   \n",
       "\n",
       "                                             hypothesis  label  \n",
       "0     Tidak terdapat analisis yang melibatkan jumlah...      2  \n",
       "1     Bangkok terpilih sebagai tuan rumah Piala Thom...      0  \n",
       "2                      Pandi sering meluncurkan domain.      1  \n",
       "3     Pada tahun 1933, penduduknya berjumlah 19.000 ...      0  \n",
       "4              Terdapat sebuah desa bernama desa Wangi.      0  \n",
       "...                                                 ...    ...  \n",
       "2196  Sinopsis film lepas Star Wars tidak diungkapka...      2  \n",
       "2197  Reruntuhan tersebut ditemukan saat penghancura...      2  \n",
       "2198             Yuko Hara tidak pernah mengambil cuti.      2  \n",
       "2199                Oerip menilai kebijakan pemerintah.      0  \n",
       "2200                            Mental positif penting.      0  \n",
       "\n",
       "[2201 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test_t = pd.DataFrame()\n",
    "df_test_t[\"premise\"] = df_test[\"premise\"]\n",
    "df_test_t[\"hypothesis\"] = df_test[\"hypothesis\"]\n",
    "df_test_t[\"label\"] = df_test[\"label\"]\n",
    "df_test_t = df_test_t.sample(frac=1).reset_index(drop=True)\n",
    "display(df_test_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa24c6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count per class test:\n",
      "0    808\n",
      "2    764\n",
      "1    629\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Count per class test:\") \n",
    "print(df_test_t['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da092653",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66688a9d",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a1737ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88b3565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompDataset(Dataset):\n",
    "    def __init__(self, df_teacher, df_student):\n",
    "        self.df_data_teacher = df_teacher\n",
    "        self.df_data_student = df_student\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # Teacher\n",
    "        sentence_teacher_1 = self.df_data_teacher.loc[index, 'premise']\n",
    "        sentence_teacher_2 = self.df_data_teacher.loc[index, 'hypothesis']\n",
    "        \n",
    "        encoded_dict_teacher = tokenizer.encode_plus(\n",
    "            sentence_teacher_1,\n",
    "            sentence_teacher_2,\n",
    "            add_special_tokens = True,\n",
    "            max_length = MAX_LEN,\n",
    "            truncation='longest_first',\n",
    "            padding = 'max_length',\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        \n",
    "        padded_token_list_teacher = encoded_dict_teacher['input_ids'][0]\n",
    "        att_mask_teacher = encoded_dict_teacher['attention_mask'][0]\n",
    "        tok_type_id_teacher = encoded_dict_teacher['token_type_ids'][0]\n",
    "        \n",
    "        target_teacher = torch.tensor([self.df_data_teacher.loc[index, 'label']])\n",
    "        lt_target_teacher = torch.LongTensor(target_teacher)\n",
    "        onehot_encoded_lbl_teacher = F.one_hot(lt_target_teacher, num_classes=3) # 3 classes: entails, neutral, contradict\n",
    "        \n",
    "        # Student\n",
    "        sentence_student_1 = self.df_data_student.loc[index, 'premise']\n",
    "        sentence_student_2 = self.df_data_student.loc[index, 'hypothesis']\n",
    "        \n",
    "        encoded_dict_student = tokenizer.encode_plus(\n",
    "            sentence_student_1,\n",
    "            sentence_student_2,\n",
    "            add_special_tokens = True,\n",
    "            max_length = MAX_LEN,\n",
    "            truncation='longest_first',\n",
    "            padding = 'max_length',\n",
    "            return_attention_mask = True,\n",
    "            return_tensors = 'pt'\n",
    "        )\n",
    "        \n",
    "        padded_token_list_student = encoded_dict_student['input_ids'][0]\n",
    "        att_mask_student = encoded_dict_student['attention_mask'][0]\n",
    "        tok_type_id_student = encoded_dict_student['token_type_ids'][0]\n",
    "        \n",
    "        target_student = torch.tensor([self.df_data_student.loc[index, 'label']])\n",
    "        lt_target_student = torch.LongTensor(target_student)\n",
    "        onehot_encoded_lbl_student = F.one_hot(lt_target_student, num_classes=3) # 3 classes: entails, neutral, contradict\n",
    "        \n",
    "        output = {\n",
    "            \"input_ids_teacher\": padded_token_list_teacher, \n",
    "            \"attention_mask_teacher\": att_mask_teacher,\n",
    "            \"token_type_ids_teacher\": tok_type_id_teacher,\n",
    "            \"lbl_teacher\": onehot_encoded_lbl_teacher,\n",
    "            \"input_ids_student\": padded_token_list_student, \n",
    "            \"attention_mask_student\": att_mask_student,\n",
    "            \"token_type_ids_student\": tok_type_id_student,\n",
    "            \"lbl_student\": onehot_encoded_lbl_student\n",
    "        }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df_data_teacher)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e52c69",
   "metadata": {},
   "source": [
    "Tokenize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7840b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_cmp = CompDataset(df_train_t, df_train_student)\n",
    "valid_data_cmp = CompDataset(df_valid_t, df_valid_student)\n",
    "test_data_cmp = CompDataset(df_test_t, df_test_student)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4494e464",
   "metadata": {},
   "source": [
    "Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8252d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data_cmp, batch_size = BATCH_SIZE)\n",
    "valid_dataloader = DataLoader(valid_data_cmp, batch_size = BATCH_SIZE)\n",
    "test_dataloader = DataLoader(test_data_cmp, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd468cb",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780b61bc",
   "metadata": {},
   "source": [
    "Transfer Learning model as per Bandyopadhyay, D., et al (2022) paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2b3da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert_student_model = BertModel.from_pretrained(\n",
    "#             MBERT_TYPE,\n",
    "#             num_labels = 3,\n",
    "#             output_hidden_states=True\n",
    "#         )\n",
    "# bert_student_model = bert_student_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0b6e27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_student = AdamW(\n",
    "#     bert_student_model.parameters(), \n",
    "#     lr=STUDENT_LRATE\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10288871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransferLearningPaper(PreTrainedModel):\n",
    "    def __init__(self, config, lambda_kld, learningrate_student):\n",
    "        super(TransferLearningPaper, self).__init__(config)\n",
    "        \n",
    "        self.bert_model_teacher = BertModel.from_pretrained(\n",
    "            MODEL_TYPE, # using pretrained mBERT in INA language\n",
    "            num_labels = 3,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    \n",
    "        self.bert_model_student = BertModel.from_pretrained(\n",
    "            MBERT_TYPE,\n",
    "            num_labels = 3,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.optimizer_student = AdamW(\n",
    "            self.bert_model_student.parameters(), \n",
    "            lr=learningrate_student\n",
    "        )\n",
    "        \n",
    "        self.linear = nn.Linear(config.hidden_size, 3)  # Linear layer\n",
    "        self.softmax = nn.Softmax(dim=1)  # Softmax activation\n",
    "        \n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.kld = nn.KLDivLoss(reduction='batchmean')\n",
    "        \n",
    "        # Initialize the weights of the linear layer\n",
    "        self.linear.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        self.linear.bias.data.zero_()\n",
    "        \n",
    "        self.lambda_kld = lambda_kld\n",
    "    \n",
    "    def forward(self, input_ids_teacher, attention_mask_teacher, token_type_ids_teacher, lbl_teacher, input_ids_student, attention_mask_student, token_type_ids_student, lbl_student):\n",
    "        # assume the label is already one-hot encoded\n",
    "        \n",
    "        self.bert_model_teacher.eval()\n",
    "        self.bert_model_student.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.bert_model_teacher(input_ids=input_ids_teacher, attention_mask=attention_mask_teacher, token_type_ids=token_type_ids_teacher)\n",
    "            outputs_student = self.bert_model_student(input_ids=input_ids_student, attention_mask=attention_mask_student, token_type_ids=token_type_ids_student)\n",
    "        \n",
    "            # take CLS token of the last hidden state\n",
    "            pooled_output_teacher = outputs_teacher[0][:, 0, :]\n",
    "            pooled_output_student = outputs_student[0][:, 0, :]\n",
    "        \n",
    "        linear_output = self.linear(pooled_output_student) # the output's logits\n",
    "        softmax_linear_output = F.log_softmax(linear_output, dim=1)\n",
    "        \n",
    "        lbl_student = lbl_student[:,0,:].float()\n",
    "        lbl_teacher = lbl_teacher[:,0,:].float()\n",
    "        softmax_linear_output = softmax_linear_output.float()\n",
    "        \n",
    "        cross_entropy_loss = self.cross_entropy(softmax_linear_output, lbl_student)\n",
    "        total_kld = self.kld(F.log_softmax(pooled_output_student, dim=1), F.softmax(pooled_output_teacher, dim=1))\n",
    "        \n",
    "        joint_loss = cross_entropy_loss + (self.lambda_kld * total_kld )\n",
    "        \n",
    "        return {\"loss\": joint_loss, \"logits\": softmax_linear_output}\n",
    "    \n",
    "    def update_param_student_model(self, loss):\n",
    "        # Doing customized backpropagation for student's model\n",
    "        self.optimizer_student.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer_student.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eddc3e95",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PretrainedConfig {\n",
      "  \"_name_or_path\": \"indojavanesenli-transfer-learning\",\n",
      "  \"finetuning_task\": \"indonesian-javanese natural language inference\",\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"ENTAIL\",\n",
      "    \"1\": \"NEUTRAL\",\n",
      "    \"2\": \"CONTRADICTION\"\n",
      "  },\n",
      "  \"label2id\": {\n",
      "    \"CONTRADICTION\": 2,\n",
      "    \"ENTAIL\": 0,\n",
      "    \"NEUTRAL\": 1\n",
      "  },\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"transformers_version\": \"4.27.3\"\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at jalaluddin94/nli_mbert were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\sufin\\anaconda3\\envs\\my-torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config = PretrainedConfig(\n",
    "    problem_type = \"single_label_classification\",\n",
    "    id2label = {\n",
    "        \"0\": \"ENTAIL\",\n",
    "        \"1\": \"NEUTRAL\",\n",
    "        \"2\": \"CONTRADICTION\"\n",
    "    },\n",
    "    label2id = {\n",
    "        \"ENTAIL\": 0,\n",
    "        \"NEUTRAL\": 1,\n",
    "        \"CONTRADICTION\": 2\n",
    "    },\n",
    "    num_labels = 3,\n",
    "    hidden_size = 768,\n",
    "    name_or_path = \"indojavanesenli-transfer-learning\",\n",
    "    finetuning_task = \"indonesian-javanese natural language inference\"\n",
    ")\n",
    "print(config)\n",
    "transferlearning_model = TransferLearningPaper(\n",
    "    config = config,\n",
    "    lambda_kld = 0.25, # antara 0.01-0.5\n",
    "    learningrate_student = STUDENT_LRATE\n",
    ")\n",
    "transferlearning_model = transferlearning_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e175723",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4490e8",
   "metadata": {},
   "source": [
    "Collect garbage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ce623e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ebef17",
   "metadata": {},
   "source": [
    "Function to compute metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5ecafed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    print(\"Computing metrics...\")\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred[:,0,:], axis=1)\n",
    "    print(\"pred:\", pred)\n",
    "    print(\"labels\", labels)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average='micro')\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average='weighted')\n",
    "    \n",
    "    print(\"f1 score:\", f1)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1_score\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7381a",
   "metadata": {},
   "source": [
    "Manual training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa7e2875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(the_model, train_data):\n",
    "    the_model.train()\n",
    "    \n",
    "    batch_loss = 0\n",
    "    \n",
    "    for batch, data in enumerate(train_data):\n",
    "        input_ids_teacher = data[\"input_ids_teacher\"].to(device)\n",
    "        attention_mask_teacher = data[\"attention_mask_teacher\"].to(device)\n",
    "        token_type_ids_teacher = data[\"token_type_ids_teacher\"].to(device)\n",
    "        lbl_teacher = data[\"lbl_teacher\"].to(device)\n",
    "        input_ids_student = data[\"input_ids_student\"].to(device)\n",
    "        attention_mask_student = data[\"attention_mask_student\"].to(device)\n",
    "        token_type_ids_student = data[\"token_type_ids_student\"].to(device)\n",
    "        lbl_student = data[\"lbl_student\"].to(device)\n",
    "        \n",
    "        output = the_model(\n",
    "            input_ids_teacher = input_ids_teacher, \n",
    "            attention_mask_teacher = attention_mask_teacher, \n",
    "            token_type_ids_teacher = token_type_ids_teacher, \n",
    "            lbl_teacher = lbl_teacher, \n",
    "            input_ids_student = input_ids_student, \n",
    "            attention_mask_student = attention_mask_student, \n",
    "            token_type_ids_student = token_type_ids_student, \n",
    "            lbl_student = lbl_student\n",
    "        )\n",
    "        \n",
    "        loss_model = output[\"loss\"]\n",
    "        batch_loss += loss_model\n",
    "        wandb.log({\"loss\": loss_model})\n",
    "        \n",
    "        # Backpropagation\n",
    "        the_model.update_param_student_model(loss_model)\n",
    "    \n",
    "    training_loss = batch_loss / BATCH_SIZE\n",
    "    \n",
    "    return training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d1f01b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(the_model, valid_data):\n",
    "    the_model.eval()\n",
    "    \n",
    "    batch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, data in enumerate(valid_data):\n",
    "            input_ids_teacher = data[\"input_ids_teacher\"].to(device)\n",
    "            attention_mask_teacher = data[\"attention_mask_teacher\"].to(device)\n",
    "            token_type_ids_teacher = data[\"token_type_ids_teacher\"].to(device)\n",
    "            lbl_teacher = data[\"lbl_teacher\"].to(device)\n",
    "            input_ids_student = data[\"input_ids_student\"].to(device)\n",
    "            attention_mask_student = data[\"attention_mask_student\"].to(device)\n",
    "            token_type_ids_student = data[\"token_type_ids_student\"].to(device)\n",
    "            lbl_student = data[\"lbl_student\"].to(device)\n",
    "\n",
    "            output = the_model(\n",
    "                input_ids_teacher = input_ids_teacher, \n",
    "                attention_mask_teacher = attention_mask_teacher, \n",
    "                token_type_ids_teacher = token_type_ids_teacher, \n",
    "                lbl_teacher = lbl_teacher, \n",
    "                input_ids_student = input_ids_student, \n",
    "                attention_mask_student = attention_mask_student, \n",
    "                token_type_ids_student = token_type_ids_student, \n",
    "                lbl_student = lbl_student\n",
    "            )\n",
    "\n",
    "            loss_model = output[\"loss\"]\n",
    "            batch_loss += loss_model\n",
    "            wandb.log({\"eval_loss\": loss_model})\n",
    "    \n",
    "        eval_loss = batch_loss / BATCH_SIZE\n",
    "    \n",
    "    return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8eb20ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_sequence(the_model, train_data, valid_data, epochs):\n",
    "    track_train_loss = []\n",
    "    track_val_loss = []\n",
    "    \n",
    "    t = trange(epochs, colour=\"green\", position=0, leave=True)\n",
    "    for ep in t:\n",
    "        training_loss = train(the_model, train_data)\n",
    "        valid_loss = validate(the_model, valid_data)\n",
    "        \n",
    "        track_train_loss.append(training_loss)\n",
    "        track_val_loss.append(valid_loss)\n",
    "        \n",
    "        t.set_description(f\"Epoch [{ep + 1}/{epochs}] - Training loss: {training_loss:.2f} Validation loss: {valid_loss:.2f}\")\n",
    "        \n",
    "        if valid_loss < min(track_val_loss) or ep + 1 == 1:\n",
    "            the_model.save_pretrained(\n",
    "                save_directory = MODEL_PATH + \"indojavanesenli-transfer-learning\"\n",
    "            )\n",
    "    return {\n",
    "        \"training_loss\": track_train_loss,\n",
    "        \"validation_loss\": track_val_loss\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "649dfa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [50/50] - Training loss: 183.34 Validation loss: 38.31: 100%|\u001b[32m█████████████████\u001b[0m| 50/50 [9:31:45<00:00, 686.11s/it]\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'training_loss': [],\n",
       " 'validation_loss': [tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0'),\n",
       "  tensor(38.3089, device='cuda:0')]}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_sequence(transferlearning_model, train_dataloader, valid_dataloader, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c124845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transferlearning_model.save_pretrained(save_directory = MODEL_PATH + \"indojavanesenli-transfer-learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b70293f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>▇▄▃▂▅▅▁▅█▅▇▅▆▅▆▆▆▄▇▅▅▆▃▃█▄▇▆▆▆▆▆▇▄▄▆▄▅▇█</td></tr><tr><td>loss</td><td>▇▄▃▄▅█▆▃▄█▃▆▅▂▇▆▅▅▂▇▆█▅▆▆█▅▅█▅▅▁▇▅▆▃▄▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval_loss</td><td>1.18867</td></tr><tr><td>loss</td><td>1.31325</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">transfer-learning-paper</strong> at: <a href='https://wandb.ai/jalaluddin-94/javanese_nli/runs/t00xi5e3' target=\"_blank\">https://wandb.ai/jalaluddin-94/javanese_nli/runs/t00xi5e3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230613_023205-t00xi5e3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05adf552",
   "metadata": {},
   "source": [
    "Training using Trainer from Huggingface (couldn't work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ccb0d24f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sufin\\anaconda3\\envs\\my-torch\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\Training\\Machine Learning\\NLP\\NLI\\wandb\\run-20230612_195156-rrjsjxh3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/jalaluddin-94/javanese_nli/runs/rrjsjxh3' target=\"_blank\">transfer-learning-paper-lambda-0.25</a></strong> to <a href='https://wandb.ai/jalaluddin-94/javanese_nli' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/jalaluddin-94/javanese_nli' target=\"_blank\">https://wandb.ai/jalaluddin-94/javanese_nli</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/jalaluddin-94/javanese_nli/runs/rrjsjxh3' target=\"_blank\">https://wandb.ai/jalaluddin-94/javanese_nli/runs/rrjsjxh3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lbl_teacher.size: torch.Size([6, 3])\n",
      "lbl_student.size: torch.Size([6, 3])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3) must match the size of tensor b (768) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 27\u001b[0m\n\u001b[0;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mMODEL_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindojavanesenli-transfer-learning/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# no\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     run_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransfer-learning-paper-lambda-0.25\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mtransferlearning_model\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     21\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my-torch\\lib\\site-packages\\transformers\\trainer.py:1633\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1630\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1631\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1632\u001b[0m )\n\u001b[1;32m-> 1633\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1638\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my-torch\\lib\\site-packages\\transformers\\trainer.py:1902\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1900\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1902\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1905\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1906\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1907\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1908\u001b[0m ):\n\u001b[0;32m   1909\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1910\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my-torch\\lib\\site-packages\\transformers\\trainer.py:2645\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2642\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2644\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2645\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2648\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my-torch\\lib\\site-packages\\transformers\\trainer.py:2677\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2675\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2676\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2677\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2678\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2679\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[26], line 52\u001b[0m, in \u001b[0;36mTransferLearningPaper.forward\u001b[1;34m(self, input_ids_teacher, attention_mask_teacher, token_type_ids_teacher, lbl_teacher, input_ids_student, attention_mask_student, token_type_ids_student, lbl_student)\u001b[0m\n\u001b[0;32m     49\u001b[0m softmax_linear_output \u001b[38;5;241m=\u001b[39m softmax_linear_output\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m     51\u001b[0m cross_entropy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_entropy(softmax_linear_output, lbl_student)\n\u001b[1;32m---> 52\u001b[0m kl_divergence_teacher \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooled_output_teacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlbl_teacher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m kl_divergence_student \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkld(F\u001b[38;5;241m.\u001b[39mlog_softmax(pooled_output_student, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), lbl_student)\n\u001b[0;32m     54\u001b[0m total_kld \u001b[38;5;241m=\u001b[39m kl_divergence_teacher \u001b[38;5;241m+\u001b[39m kl_divergence_student\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:465\u001b[0m, in \u001b[0;36mKLDivLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkl_div\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_target\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\my-torch\\lib\\site-packages\\torch\\nn\\functional.py:2916\u001b[0m, in \u001b[0;36mkl_div\u001b[1;34m(input, target, size_average, reduce, reduction, log_target)\u001b[0m\n\u001b[0;32m   2913\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2914\u001b[0m         reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[1;32m-> 2916\u001b[0m reduced \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkl_div\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_target\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_target\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatchmean\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2919\u001b[0m     reduced \u001b[38;5;241m=\u001b[39m reduced \u001b[38;5;241m/\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (768) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=MODEL_PATH + \"indojavanesenli-transfer-learning/\",\n",
    "#     save_strategy=\"no\", # no\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     logging_strategy=\"epoch\",\n",
    "#     learning_rate=L_RATE,\n",
    "#     per_device_train_batch_size=BATCH_SIZE,\n",
    "#     per_device_eval_batch_size=BATCH_SIZE,\n",
    "#     overwrite_output_dir=True,\n",
    "#     num_train_epochs=NUM_EPOCHS,\n",
    "#     weight_decay=LAMBDA_L2,\n",
    "#     hub_token=HF_TOKEN,\n",
    "#     report_to=\"wandb\",\n",
    "#     gradient_accumulation_steps=1000,\n",
    "#     push_to_hub=False,\n",
    "#     run_name=\"transfer-learning-paper-lambda-0.25\"\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=transferlearning_model.to(device),\n",
    "#     args=training_args,\n",
    "#     train_dataset=train_data_cmp,\n",
    "#     eval_dataset=valid_data_cmp,\n",
    "#     compute_metrics=compute_metrics\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e082f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fin_eval = trainer.evaluate()\n",
    "# wandb.log({\"f1_score\": fin_eval[\"eval_f1_score\"], \"eval_loss\": fin_eval[\"eval_loss\"], \"accuracy\": fin_eval[\"eval_accuracy\"], \"precision\": fin_eval[\"eval_precision\"], \"recall\": fin_eval[\"eval_recall\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af12bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dadd42e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-torch",
   "language": "python",
   "name": "my-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
